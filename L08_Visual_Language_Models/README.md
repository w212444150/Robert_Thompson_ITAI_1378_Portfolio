# Visual Language Models (VLMs)

## Problem Statement
Exploring Multimodal AI systems that understand both images and text for tasks like Zero-Shot classification and VQA.

## Approach
- Implemented **CLIP** for Zero-Shot classification.
- Used **BLIP** for Image Captioning and Visual Question Answering (VQA).

## Key Findings
- **Zero-Shot:** VLMs can classify images into categories they were never explicitly trained on.
- **Multimodal:** demonstrated the ability to search for images using natural language queries.

## Technologies Used
- Hugging Face Transformers
- PyTorch
- Pillow
